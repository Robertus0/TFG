{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'D:\\Clase\\TFG\\env\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os\n",
    "from transformers import DetrConfig, DetrForObjectDetection, DetrFeatureExtractor\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder,  train_json_path, test_json_path, feature_extractor, train=True):\n",
    "        #ann_file = os.path.join(img_folder, \"custom_train.json\" if train else \"custom_val.json\")\n",
    "        if train:\n",
    "            ann_file = train_json_path\n",
    "        else:\n",
    "            ann_file = test_json_path\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\feature_extraction_detr.py:28: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:780: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_img_path = \"..\\\\data\\\\Zona\\\\Dataset\\\\images\\\\\"\n",
    "train_path = \"..\\\\data\\\\Zona\\\\Dataset\\\\annotations\\\\annotations.json\"\n",
    "test_path = \"\"\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "train_dataset = CocoDetection(train_img_path, train_path, test_path, feature_extractor = feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CocoDetection\n",
       "    Number of datapoints: 526\n",
       "    Root location: ..\\data\\Zona\\Dataset\\images\\"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrConfig, DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, lr_backbone, weight_decay, id2label, train_dataloader, val_dataloader):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", \n",
    "                                                             num_labels=len(id2label),\n",
    "                                                             ignore_mismatched_sizes=True)\n",
    "         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "         self.t_dataloader = train_dataloader\n",
    "         self.v_dataloader = val_dataloader\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "         \n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "         \n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "         \n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "         \n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "         \n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "         \n",
    "        return self.t_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "         \n",
    "        return self.v_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\feature_extraction_detr.py:28: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:780: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 41.5 M\n",
      "-------------------------------------------------\n",
      "41.3 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.5 M    Total params\n",
      "166.037   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:886: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\feature_extraction_detr.py:28: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:780: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:970: FutureWarning: This method is deprecated and will be removed in v4.27.0. Please use pad instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    111\u001b[0m train_img_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mdata\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mZona\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDataset\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mimages\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 112\u001b[0m Train(train_img_path, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmain()\n",
      "Cell \u001b[1;32mIn[5], line 106\u001b[0m, in \u001b[0;36mTrain.main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mClase\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTFG\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfirt_model.ckpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m train_dataset, test_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_dataset(train_path, test_path)\n\u001b[1;32m--> 106\u001b[0m _, trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(train_dataset, test_dataset)\n\u001b[0;32m    107\u001b[0m trainer\u001b[39m.\u001b[39msave_checkpoint(model_path)\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m, in \u001b[0;36mTrain.train\u001b[1;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39m#PATH = '/Users/.../aa.ckpt'\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m#model = model.load_from_checkpoint(PATH,lr=self.lr, lr_backbone=self.lr_backbone, weight_decay=self.weight_decay, id2label = id2label, train_dataloader = train_dataloader, val_dataloader = val_dataloader)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m trainer \u001b[39m=\u001b[39m Trainer( max_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps, gradient_clip_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_clip_val)\n\u001b[1;32m---> 93\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model)\n\u001b[0;32m     95\u001b[0m \u001b[39m#-----\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation(val_dataset, val_dataloader, model)\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[0;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    555\u001b[0m     ckpt_path,\n\u001b[0;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    558\u001b[0m )\n\u001b[1;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:976\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m--> 976\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m    977\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m    978\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1005\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1002\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1004\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1007\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1009\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[0;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32md:\\Clase\\TFG\\env\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m    377\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m, in \u001b[0;36mDetr.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m---> 52\u001b[0m    loss, loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommon_step(batch, batch_idx)     \n\u001b[0;32m     53\u001b[0m    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mvalidation_loss\u001b[39m\u001b[39m\"\u001b[39m, loss)\n\u001b[0;32m     54\u001b[0m    \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mDetr.common_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m pixel_values \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     29\u001b[0m pixel_mask \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 30\u001b[0m labels \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m     32\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(pixel_values\u001b[39m=\u001b[39mpixel_values, pixel_mask\u001b[39m=\u001b[39mpixel_mask, labels\u001b[39m=\u001b[39mlabels)\n\u001b[0;32m     34\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m pixel_values \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     29\u001b[0m pixel_mask \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_mask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 30\u001b[0m labels \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39;49mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m     32\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(pixel_values\u001b[39m=\u001b[39mpixel_values, pixel_mask\u001b[39m=\u001b[39mpixel_mask, labels\u001b[39m=\u001b[39mlabels)\n\u001b[0;32m     34\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DetrFeatureExtractor\n",
    "from pytorch_lightning import Trainer\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets_helper import get_coco_api_from_dataset\n",
    "from datasets_helper.coco_eval import CocoEvaluator\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, train_img_path, test_img_path):\n",
    "        self.train_img_path = train_img_path\n",
    "        self.test_img_path = test_img_path\n",
    "        self.feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        self.train_batch_size = 4\n",
    "        self.test_batch_size = 2\n",
    "        self.gpus = 0\n",
    "        self.lr = 0.0001\n",
    "        self.lr_backbone = 1e-05\n",
    "        self.weight_decay = 0.0001\n",
    "        self.max_steps = 6000\n",
    "        self.gradient_clip_val = 0.1\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \n",
    "        feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        pixel_values = [item[0] for item in batch]\n",
    "        encoding = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "        labels = [0 for item in batch]\n",
    "        batch = {}\n",
    "        batch['pixel_values'] = encoding['pixel_values']\n",
    "        batch['pixel_mask'] = encoding['pixel_mask']\n",
    "        batch['labels'] = labels\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def create_dataset(self, train_path, test_path):\n",
    "\n",
    "        #feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        train_dataset = CocoDetection(self.train_img_path, train_path, test_path, feature_extractor = self.feature_extractor)\n",
    "        val_dataset = CocoDetection(self.test_img_path, train_path, test_path, feature_extractor = self.feature_extractor, train=False)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def evaluation(self, val_dataset, val_dataloader, model):\n",
    "\n",
    "        base_ds = get_coco_api_from_dataset(val_dataset)\n",
    "        iou_types = ['bbox']\n",
    "        coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        print(\"Running evaluation...\")\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "            # get the inputs\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "            orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "            results = self.feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to COCO api\n",
    "            res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "            coco_evaluator.update(res)\n",
    "\n",
    "        coco_evaluator.synchronize_between_processes()\n",
    "        coco_evaluator.accumulate()\n",
    "        coco_evaluator.summarize()\n",
    "    \n",
    "    \n",
    "    def train(self, train_dataset, val_dataset):\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, collate_fn=Train.collate_fn, batch_size = self.train_batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, collate_fn=Train.collate_fn, batch_size = self.test_batch_size)\n",
    "        #batch = next(iter(train_dataloader))\n",
    "        cats = train_dataset.coco.cats\n",
    "        id2label = {k: v['name'] for k,v in cats.items()}\n",
    "        model = Detr(lr=self.lr, lr_backbone=self.lr_backbone, weight_decay=self.weight_decay, id2label = id2label, train_dataloader = train_dataloader, val_dataloader = val_dataloader)\n",
    "        #PATH = '/Users/.../aa.ckpt'\n",
    "        #model = model.load_from_checkpoint(PATH,lr=self.lr, lr_backbone=self.lr_backbone, weight_decay=self.weight_decay, id2label = id2label, train_dataloader = train_dataloader, val_dataloader = val_dataloader)\n",
    "        trainer = Trainer( max_steps = self.max_steps, gradient_clip_val = self.gradient_clip_val)\n",
    "        trainer.fit(model)\n",
    "\n",
    "        #-----\n",
    "        self.evaluation(val_dataset, val_dataloader, model)\n",
    "        \n",
    "        return model, trainer\n",
    "        \n",
    "    def main(self):\n",
    "        \n",
    "        train_path = \"..\\\\data\\\\Zona\\\\Dataset\\\\annotations\\\\annotations.json\"\n",
    "        test_path = \"..\\\\data\\\\Zona\\\\Dataset\\\\annotations\\\\annotations.json\"\n",
    "        model_path = \"D:\\Clase\\TFG\\model\\\\firt_model.ckpt\"\n",
    "        train_dataset, test_dataset = self.create_dataset(train_path, test_path)\n",
    "        _, trainer = self.train(train_dataset, test_dataset)\n",
    "        trainer.save_checkpoint(model_path)\n",
    "        \n",
    "        return\n",
    "\n",
    "train_img_path = \"..\\\\data\\\\Zona\\\\Dataset\\\\images\\\\\"\n",
    "Train(train_img_path, \"\").main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\rober/.cache\\torch\\hub\\facebookresearch_detr_main\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'D:\\Clase\\TFG\\env\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_dataset, get_coco_api_from_dataset\n",
    "from engine import evaluate, train_one_epoch\n",
    "from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DETR(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_embed): Linear(in_features=256, out_features=92, bias=True)\n",
       "  (bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (query_embed): Embedding(100, 256)\n",
       "  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(args):\n",
    "    model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "    \n",
    "    return model, None, None\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "    print(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = 1000\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    model, criterion, postprocessors = build_model(args)\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('number of params:', n_parameters)\n",
    "\n",
    "    param_dicts = [\n",
    "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "            \"lr\": args.lr_backbone,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                  weight_decay=args.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "    dataset_train = build_dataset(image_set='train', args=args)\n",
    "    dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "    if args.distributed:\n",
    "        sampler_train = DistributedSampler(dataset_train)\n",
    "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                   collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
    "\n",
    "    if args.dataset_file == \"coco_panoptic\":\n",
    "        # We also evaluate AP during panoptic training, on original coco DS\n",
    "        coco_val = datasets.coco.build(\"val\", args)\n",
    "        base_ds = get_coco_api_from_dataset(coco_val)\n",
    "    else:\n",
    "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            args.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                              data_loader_val, base_ds, device, args.output_dir)\n",
    "        if args.output_dir:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "        return\n",
    "\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            sampler_train.set_epoch(epoch)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "            args.clip_max_norm)\n",
    "        lr_scheduler.step()\n",
    "        if args.output_dir:\n",
    "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "            # extra checkpoint before LR drop and every 100 epochs\n",
    "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
    "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                utils.save_on_master({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'args': args,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        test_stats, coco_evaluator = evaluate(\n",
    "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "        )\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                     'epoch': epoch,\n",
    "                     'n_parameters': n_parameters}\n",
    "\n",
    "        if args.output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "            # for evaluation logs\n",
    "            if coco_evaluator is not None:\n",
    "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "                if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                    filenames = ['latest.pth']\n",
    "                    if epoch % 50 == 0:\n",
    "                        filenames.append(f'{epoch:03}.pth')\n",
    "                    for name in filenames:\n",
    "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                                   output_dir / \"eval\" / name)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otro intento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'detectron2'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///D:/Clase/TFG/src/detectron2\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=7.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (3.7.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (2.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (2.2.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (2.2.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (4.65.0)\n",
      "Requirement already satisfied: tensorboard in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (2.12.1)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (0.1.9)\n",
      "Requirement already satisfied: omegaconf>=2.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (1.3.2)\n",
      "Requirement already satisfied: black in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (21.4b2)\n",
      "Requirement already satisfied: packaging in d:\\clase\\tfg\\env\\lib\\site-packages (from detectron2==0.6) (23.0)\n",
      "Requirement already satisfied: numpy in d:\\clase\\tfg\\env\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in d:\\clase\\tfg\\env\\lib\\site-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
      "Requirement already satisfied: portalocker in d:\\clase\\tfg\\env\\lib\\site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (4.39.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
      "Requirement already satisfied: colorama in d:\\clase\\tfg\\env\\lib\\site-packages (from tqdm>4.29.0->detectron2==0.6) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in d:\\clase\\tfg\\env\\lib\\site-packages (from black->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: regex>=2020.1.8 in d:\\clase\\tfg\\env\\lib\\site-packages (from black->detectron2==0.6) (2023.3.23)\n",
      "Requirement already satisfied: pathspec<1,>=0.8.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from black->detectron2==0.6) (0.11.1)\n",
      "Requirement already satisfied: click>=7.1.2 in d:\\clase\\tfg\\env\\lib\\site-packages (from black->detectron2==0.6) (8.1.3)\n",
      "Requirement already satisfied: appdirs in d:\\clase\\tfg\\env\\lib\\site-packages (from black->detectron2==0.6) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.10.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from black->detectron2==0.6) (0.10.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.40.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.2.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (58.1.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (4.22.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.17.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.53.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in d:\\clase\\tfg\\env\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\clase\\tfg\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->detectron2==0.6) (3.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\clase\\tfg\\env\\lib\\site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (6.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\clase\\tfg\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\clase\\tfg\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\clase\\tfg\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\clase\\tfg\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.2)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\clase\\tfg\\env\\lib\\site-packages (from portalocker->iopath<0.1.10,>=0.1.7->detectron2==0.6) (305)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\clase\\tfg\\env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\clase\\tfg\\env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
      "Installing collected packages: detectron2\n",
      "  Running setup.py develop for detectron2\n",
      "Successfully installed detectron2-0.6\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2.git\n",
    "!python -m pip install -e detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Clase\\TFG\\env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'D:\\Clase\\TFG\\env\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import detectron2\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.modeling import build_model\n",
    "\n",
    "# Registra los datasets\n",
    "register_coco_instances(\"piscinas_entrenamiento\", {}, \"ruta/al/annotation.json\", \"ruta/al/images\")\n",
    "register_coco_instances(\"piscinas_validacion\", {}, \"ruta/al/annotation.json\", \"ruta/al/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0053dad21f9b7f60dfce80eb7ba2a04e7d67243aea089bdeb60e0e192808ba8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
